#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from ast import Not
import enum
from statistics import mean
import sys

# from farm_code_W.reconstruct.train.Iteration_train import Accuracy
sys.path.append("../")
import os
import numpy as np
import random
import torch

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report
from torch.utils.data.dataset import Dataset
import time
import glob

import os
import torch



n=1
Accuracy = 0

Model_Dir = '/home/kumarv/pravirat/Realsat_labelling/ASIA/models/optimal'
## Parameters
experiment_id = '10models_X'
learning_rate = 0.001
patch_size = 64
inchannels = 1
outchannels = 2 
# 0 represents unconfirmed
# 1 represents confirmed as farm
# 2 represents confirmed as not farm
no_epochs = 300
lower_lim = 350
upper_lim = 400
batch_size = 256
continent_no = 1
bias =  0.0000000035
farm_label_array_path = '/home/kumarv/pravirat/Realsat_labelling/ASIA/farm_labels_asia.npy'
warped_data_path = '/home/kumarv/pravirat/Realsat_labelling/WARPED_DATA/350_400_stage2_warped_64x64/'
continent_info_array_path = '/home/kumarv/pravirat/Realsat_labelling/continent_info.npy'
 
# create model directory if it doesnt exist
if not os.path.exists(Model_Dir):
    os.makedirs(Model_Dir)


# define deep learning model architecture


class CNN_reconstruct(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CNN_reconstruct, self).__init__()     
        self.conv1_1 = torch.nn.Conv2d(in_channels, 16, 3, padding=1)
        self.conv1_2 = torch.nn.Conv2d(16, 16, 3, padding=1)
        self.conv2_1 = torch.nn.Conv2d(16, 32, 3, padding=1)
        self.conv2_2 = torch.nn.Conv2d(32, 32, 3, padding=1)
        self.conv3_1 = torch.nn.Conv2d(32, 64, 3, padding=1)
        self.conv3_2 = torch.nn.Conv2d(64, 64, 3, padding=1)
        self.fc = torch.nn.Linear(4096, 1024)

        self.upfc = torch.nn.Linear(1024, 4096)
        self.unpool3 = torch.nn.ConvTranspose2d(64 , 64, kernel_size=2, stride=2)
        self.upconv3_1 = torch.nn.Conv2d(64, 64, 3, padding=1)
        self.upconv3_2 = torch.nn.Conv2d(64, 32, 3, padding=1)
        self.unpool2 = torch.nn.ConvTranspose2d(32 , 32, kernel_size=2, stride=2)
        self.upconv2_1 = torch.nn.Conv2d(32, 32, 3, padding=1)
        self.upconv2_2 = torch.nn.Conv2d(32, 16, 3, padding=1)
        self.unpool1 = torch.nn.ConvTranspose2d(16 , 16, kernel_size=2, stride=2)
        self.upconv1_1 = torch.nn.Conv2d(16, 16, 3, padding=1)
        self.upconv1_2 = torch.nn.Conv2d(16, in_channels, 3, padding=1)
        
        self.classification_layer = torch.nn.Linear(1024, out_channels)

        self.maxpool = torch.nn.MaxPool2d(2)
        self.relu = torch.nn.ReLU(inplace=True)
        self.softmax = torch.nn.Softmax()
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight)
        
    def forward(self,x):
        x = x.view(-1, 1, patch_size, patch_size)

        conv1 = self.maxpool(self.relu(self.conv1_2(self.relu(self.conv1_1(x)))))
        conv2 = self.maxpool(self.relu(self.conv2_2(self.relu(self.conv2_1(conv1)))))
        conv3 = self.maxpool(self.relu(self.conv3_2(self.relu(self.conv3_1(conv2)))))                                     
        fc = self.relu(self.fc(conv3.view(-1,4096)))

        classification = self.classification_layer(fc)

        upfc = self.relu(self.upfc(fc))
        upconv3 = self.relu(self.upconv3_2(self.relu(self.upconv3_1(self.unpool3(upfc.view(-1,64,8,8))))))
        upconv2 = self.relu(self.upconv2_2(self.relu(self.upconv2_1(self.unpool2(upconv3)))))
        out = self.upconv1_2(self.relu(self.upconv1_1(self.unpool1(upconv2))))
        out = out.view(-1, 1, patch_size, patch_size)
        
        return classification,out

def mse_loss(input_image, target, ignored_index, reduction):
    mask = input_image == ignored_index
    out = (input_image[~mask]-target[~mask])**2
    if reduction == "mean":
        return out.mean()
    elif reduction == "None":
        return out
                                                    
# define function for creating fraction map and label for a particular ID
def create_frac_map_and_label(ID,label_array):
#     ID = path.split('/')[-1].split('_')[-4] 
    strID='test'
    if(len(str(ID))!=6):
        complete = 6-len(str(ID))
        strID = '0'*complete+str(ID)
    else:
        strID = str(ID)
    image = np.load(warped_data_path + 'ID_' + strID + '_orbit_updated_warped.npy')
    
    # converting labels to binary, i.e land or water
    image[image == 1] = 0 
    image[image == 2] = 1 
        
    frac_map_image = np.mean(image,axis = 0)

    frac_map = np.array(frac_map_image).astype(np.float32)
    label_image = label_array[int(ID)]

    return frac_map, label_image, ID

# create dataloader class 
class CLASSIFIER(Dataset):

    def __init__(self, frac_maps, label_images, IDs):
        self.frac_maps = frac_maps
        self.label_images = label_images
        self.IDs = IDs

    def __len__(self):
        return len(self.label_images)

    def __getitem__(self, index):
        return self.frac_maps[index], self.label_images[index],self.IDs[index]

# define loss function
criterion = torch.nn.CrossEntropyLoss()

criterion_classification = torch.nn.CrossEntropyLoss()


# def testAccuracy():
    
#     model.eval()
#     accuracy = 0.0
#     total = 0.0
    
#     with torch.no_grad():
#         for data in test_loader:
#             images, labels = data
#             # run the model on the test set to predict labels
#             outputs = model(images)
#             # the label with the highest energy will be our prediction
#             _, predicted = torch.max(outputs.data, 1)
#             total += labels.size(0)
#             accuracy += (predicted == labels).sum().item()
    
#     # compute the accuracy over all test images
#     accuracy = (100 * accuracy / total)
#     return(accuracy)


# criterion = torch.nn.BCELoss()

# build model
print("BUILD MODEL")
model = CNN_reconstruct(in_channels=inchannels, out_channels=outchannels)
model = model.to('cuda')
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# get list of paths to use for training 
farm_label_array = np.load(farm_label_array_path)

paths_list = glob.glob(os.path.join(warped_data_path + '*.npy')) # gets all paths

#subset paths that lie in continent of interest
continent_info = np.load(continent_info_array_path)
continent_path_list = []
continent_ID_list = []
for path in paths_list:
    ID = path.split('/')[-1].split('_')[-4]
    if(continent_info[int(ID)] == continent_no):
        continent_path_list.append(path)
        ## get all ID from certain continent
        continent_ID_list.append(int(ID))

print("EXCLUDE HARD TO DEFINE=============================================================")
print(len(continent_ID_list))

# cantdifine = [720690,730116,691971,726576,684668,709692,605971,723206,609487,619521,650182,618360,739550,602847,619168,674333,649051,605654,623910,608219,708037,607852,649654,612060,605485,540938,607212,690253,613660,675170,693737,689493,613210,698782,731414,729211,615576,611416,681027,609725,688859,605977,614444,669402,683473,738677,701327,611922,719753,615201,732522,612516,539334,713031,713419,615553,732651,683009,717825, 717829, 734214, 616975, 720406, 730137, 734746, 676379, 655900, 676383, 649253, 677927, 683566, 601138, 601652, 601653, 703551, 522323, 684630, 611930, 692322, 618086, 737383, 733288, 678508, 687724, 691828, 701046, 709752, 637051, 648827, 641150, 619135, 730242, 689802, 616590, 648850, 739484, 670876, 689310, 615588, 692904, 602283, 716973, 603315, 610486, 613566, 675016, 680146, 670936, 611032, 701149, 605410, 606951, 732907, 607478, 636150, 730361, 703738, 695037, 601348, 700188, 652068, 602928, 604980, 710979, 721228, 602957, 650604, 650104, 638840, 702336, 601987, 649604, 605583, 614800, 622486, 739226, 652193, 691107, 730533, 724903, 718248, 603580, 638403, 683459, 700872, 603091, 668642, 611813, 719340, 689652, 719348, 635898]
cantdifine = [194625,194617,193765,191053,188040,187109,186803,675036,685465,693703,694365,694667,721984,31445,30094,19054,294610,293716,283563,282272,277527,257707,235017,234832,230203,221518,20062,75008,135971,105732,88707,134827,90972,92343,90994,140623,140495,294610,293716,135086,]

cantdifine = list(set(cantdifine))

continent_ID_list = [x for x in continent_ID_list if x not in cantdifine]
print(len(continent_ID_list))

print("=============================================================")

farm_conti_list1 = []
# farm_conti_list2 = []


torch.cuda.empty_cache()
not_farm_conti_list = []

# get IDs where we know it is farm and subset those poths
farm_IDs = (np.where(farm_label_array == 1)[0]).tolist() # has all the IDs that are farms
# farm_IDs = [187073,31629,31612,31390,31365,30375,29964,29955,29830,29712,29333,19857,19723,19575,19529,19528,19337,18918,18168,29524, 29756, 29931, 31723, 31741, 31916, 32113, 32693,38858,41846, 44883, 45585, 46180, 51861, 51863, 52927,53489, 53729, 53787, 54103, 54163, 54184, 54248, 54679, 54706, 54757, 54764, 54791, 55016, 55082, 55084, 55097, 55106, 60175, 60198, 60369, 60406, 60421, 60430, 60767, 60777, 61383, 61529, 61607, 61737, 61931, 62107, 62178, 62246, 62280, 62308, 62434, 62697, 62876, 62973, 63015, 63116, 53729, 65237, 74110,75783, 76276, 76620, 77551, 77704, 77741, 79892, 80704, 80718, 81225, 84225, 84529, 84578, 84823, 85749, 86188, 86432, 86750, 87101, 87199,90465, 90961,97104,107776, 108461, 108687, 108798, 108908, 108959, 109002, 109035, 109942, 112250, 112488, 112973, 112981, 113567, 114128, 114484, 114486, 114574, 114810, 115019, 115124, 115361, 115843, 116183, 116231, 116555, 117639, 118052, 118114, 124958, 125830, 126187, 126525, 126663, 127039, 127242, 130058, 130314, 130489, 130822, 130850, 133987, 134197, 140791, 154756]
# farm_IDs = [42634,41903,41897,41130,41843,40824,43516,40274,45053,44797,41392,166892,169128,169117,39217,169413,169439,169465,169236,169496,169503,40344,169633,169659,169742,169744,172123,181059,187745,31629,31612,31390,31365,30375,29964,29955,29830,29712,29333,19857,19723,19575,19529,19528,19337,18918,18168,29524, 29756, 29931, 31723, 31741, 31916, 32113, 32693,38858,41846, 44883, 45585, 46180, 51861, 51863, 52927,53489, 53729, 53787, 54103, 54163, 54184, 54248, 54679, 54706, 54757, 54764, 54791, 55016, 55082, 55084, 55097, 55106, 60175, 60198, 60369, 60406, 60421, 60430, 60767, 60777, 61383, 61529, 61607, 61737, 61931, 62107, 62178, 62246, 62280, 62308, 62434, 62697, 62876, 62973, 63015, 63116, 53729, 65237, 74110,75783, 76276, 76620, 77551, 77704, 77741, 79892, 80704, 80718, 81225, 84225, 84529, 84578, 84823, 85749, 86188, 86432, 86750, 87101, 87199,90465, 90961,97104,107776, 108461, 108687, 108798, 108908, 108959, 109002, 109035, 109942, 112250, 112488, 112973, 112981, 113567, 114128, 114484, 114486, 114574, 114810, 115019, 115124, 115361, 115843, 116183, 116231, 116555, 117639, 118052, 118114, 124958, 125830, 126187, 126525, 126663, 127039, 127242, 130058, 130314, 130489, 130822, 130850, 133987, 134197, 140791, 154756]
farm_IDs = [283505,269563,269611,269852,269990,270315,270319,270582,270588,270627,270900,270939,282082,282066,282105,266896,267058,267089,267338,267492,267621,267664,267713,267889,268239,268626,269100,269458,265503,265563,265685,265704,265745,265922,266131,266237,266471,266481,266528,266576,266713,266645,266775,188536,193338,193658,193753,193913,235636,235640,235644,235765,239369,249065,261211,261289,261510,261597,261613,261740,261775,261790,262242,262385,262895,262950,263132,263275,263279,263393,263438,263485,263538,263624,263690,263764,263796,263885,263947,263972,263995,264335,264406,264649,264674,264791,264902,264910,264968,265064,265244,265377,265483,265487,188248,42634,41903,41897,41130,41843,40824,43516,40274,45053,44797,41392,166892,169128,169117,39217,169413,169439,169465,169236,169496,169503,40344,169633,169659,169742,169744,172123,181059,187745,31629,31612,31390,31365,30375,29964,29955,29830,29712,29333,19857,19723,19575,19529,19528,19337,18918,18168,29524, 29756, 29931, 31723, 31741, 31916, 32113, 32693,38858,41846, 44883, 45585, 46180, 51861, 51863, 52927,53489, 53729, 53787, 54103, 54163, 54184, 54248, 54679, 54706, 54757, 54764, 54791, 55016, 55082, 55084, 55097, 55106, 60175, 60198, 60369, 60406, 60421, 60430, 60767, 60777, 61383, 61529, 61607, 61737, 61931, 62107, 62178, 62246, 62280, 62308, 62434, 62697, 62876, 62973, 63015, 63116, 53729, 65237, 74110,75783, 76276, 76620, 77551, 77704, 77741, 79892, 80704, 80718, 81225, 84225, 84529, 84578, 84823, 85749, 86188, 86432, 86750, 87101, 87199,90465, 90961,97104,107776, 108461, 108687, 108798, 108908, 108959, 109002, 109035, 109942, 112250, 112488, 112973, 112981, 113567, 114128, 114484, 114486, 114574, 114810, 115019, 115124, 115361, 115843, 116183, 116231, 116555, 117639, 118052, 118114, 124958, 125830, 126187, 126525, 126663, 127039, 127242, 130058, 130314, 130489, 130822, 130850, 133987, 134197, 140791, 154756]

# not_farm_IDs = (np.where(farm_label_array == 2)[0]).tolist() # has all the IDs that are farms

lenoffarmlist = len(farm_IDs)
print(lenoffarmlist)
# not_farm_IDs = not_farm_IDs[:lenoffarmlist]
# print(len(not_farm_IDs))
for ID in continent_ID_list:
    # if ID in not_farm_IDs:
    #     farm_conti_list2.append(ID)
    if ID in farm_IDs:
        farm_conti_list1.append(ID)
        
    else:
        not_farm_conti_list.append(ID)


no_conti_farm_IDs = abs(len(farm_conti_list1))
# print(no_conti_farm_IDs)



## start training 10 different model from here
## so each loop when shuffle pick first several non-farm ID as train sample,
## these IDs will be remove from the not_farm_conti_list
## So for the future loop we will must select totally new IDs as train sample


train_loss = []
train_balance = []



while(Accuracy<0.99):

    experiment_id = str(n)
    not_s = []
    random.shuffle(not_farm_conti_list)

    # store the final IDs into a list
    final_IDs = []
    for ID in farm_conti_list1:
        final_IDs.append(ID)

    for i,ID in enumerate(not_farm_conti_list):
        if(i == no_conti_farm_IDs):
            break
        final_IDs.append(ID)
        not_s.append(ID)

    not_farm_conti_list = not_farm_conti_list[(len(farm_conti_list1)):]

    random.shuffle(final_IDs)
    frac_map_images_list = []
    label_images_list = []

    for ID in final_IDs:
        frac_map_image, label_image, ID = create_frac_map_and_label(ID,farm_label_array)            
        frac_map_images_list.append(frac_map_image)
        label_images_list.append(label_image)

    print(len(final_IDs))


    # print(len(frac_map_images_list))
    # print(frac_map_images_list[0].shape)

    ## train model
    print("TRAIN MODEL")
    print("MODEL: ",n)


    data = CLASSIFIER(frac_map_images_list, label_images_list,final_IDs)
    data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=0)
    no_epochs_monitor = 1
    for epoch in range(1,no_epochs+1):
        
        model.train()

        train_time_start = time.time()
        epoch_loss = 0
        epoch_loss_ce = 0
        epoch_loss_recon = 0
    

        for batch, [frac_map_batch, label_image_batch,ID_batch] in enumerate(data_loader):
            classification,out = model(frac_map_batch.to('cuda').float()) # gets output for a batch
            label_batch = label_image_batch.type(torch.long).to('cuda') # gets the labels for that batch
            batch_loss_ce = criterion_classification(classification, label_batch) # calculates the loss for that batch
            frac_map_batch = frac_map_batch.to('cuda').float()
            batch_loss_recon = torch.mean(torch.sum(mse_loss(input_image = out, target = frac_map_batch,ignored_index = 0,reduction = 'None')))
            if(no_epochs<180):
                batch_loss = batch_loss_ce
            else:
                batch_loss = bias*batch_loss_recon+batch_loss_ce
            optimizer.zero_grad()
            batch_loss.backward()
            optimizer.step()
            epoch_loss += batch_loss.item()
            epoch_loss_ce += batch_loss_ce.item()
            epoch_loss_recon += batch_loss_recon.item()

        epoch_loss = epoch_loss/(batch+1)
        epoch_loss_ce = epoch_loss_ce/(batch+1)
        epoch_loss_recon = epoch_loss_recon/(batch+1)
        epoch_balance = float(epoch_loss_ce/(epoch_loss_recon*bias))
        # print(epoch,"LOSS: ",epoch_loss,epoch_loss_recon)
        # print("\n")
    model.eval()
    torch.save(model.state_dict(), os.path.join(Model_Dir, str(experiment_id) +".pt"))

# ========================= ========================= ========================= =========================
    farm_label_array = np.load(farm_label_array_path)
    totallist1= farm_label_array.tolist()
    for i in range(1000000):
        totallist1[i] = 0
    final_IDs = continent_ID_list.copy()
    frac_map_images_list = []
    label_images_list = []
    print("begin to generate the images")
    for ID in final_IDs:
        frac_map_image, label_image, ID = create_frac_map_and_label(ID,farm_label_array)            
        frac_map_images_list.append(frac_map_image)
        label_images_list.append(label_image)

    print("start to run (image prepaired)")
    model.eval()
    data = CLASSIFIER(frac_map_images_list, label_images_list,final_IDs)
    data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=1, shuffle=False, num_workers=0)
    preds = []
    labels = []
    IDs_all = []
    for batch, [frac_map_batch, label_image_batch,ID_batch] in enumerate(data_loader):
        classification,out = model(frac_map_batch.to('cuda').float()) # gets output for a batch
        label_batch = label_image_batch.type(torch.long).to('cuda') # gets the labels for that batch
        out_label_batch = torch.argmax(torch.nn.functional.softmax(classification, dim=1), dim=1)
        preds  = list(preds)
        preds.append(out_label_batch.detach().cpu().numpy())
        labels.append(label_batch.cpu().numpy())
        IDs_all.append(ID_batch)
    pred_array = np.concatenate(preds, axis=0)
    ID_array = np.concatenate(IDs_all, axis=0)

    for i,d in enumerate(ID_array):
        if pred_array[i] == 1:
            totallist1[d] += pred_array[i]  

    level1=[]
    counting1 = 0

    for i,d in enumerate(totallist1):
        if(d == 1):
            counting1+=1
            level1.append(i)
    # farm_IDs = [534608, 537912, 574403, 601125, 601319, 602094, 608423, 608549, 611798, 613277, 613785, 614864, 616088, 616443, 616923, 616953, 617923, 619367, 619690, 619757, 619856, 620044, 620065, 620192, 620200, 620402, 622482, 622484, 622514, 624070, 624991, 625246, 633953, 635928, 635951, 637361, 637445, 640120, 648481, 648575, 648578, 648798, 652382, 652386, 652583, 655922, 672102, 672241, 673940, 673966, 674749, 675676, 676148, 676199, 676237, 679056, 679065, 680338, 680826, 681055, 681148, 681282, 681427, 681443, 681453, 681523, 681524, 681535, 681585, 681652, 681656, 681723, 681749, 681765, 681790, 681844, 681993, 682019, 682028, 682029, 682105, 682156, 682296, 682303, 682362, 682368, 682370, 682371, 682451, 682458, 682462, 682472, 682510, 682512, 682545, 682584, 682677, 682785, 682803, 682842, 682868, 682946, 682979, 682984, 682986, 683008, 683064, 683068, 683116, 683136, 683155, 683182, 683208, 683311, 683338, 683361, 683492, 683606, 683728, 683741, 683756, 683774, 683783, 683799, 683813, 683842, 683846, 683850, 683881, 683891, 683893, 683922, 684004, 684023, 684032, 684116, 684199, 684227, 684230, 684240, 684258, 684279, 684339, 684425, 684547, 684571, 684572, 684658, 684688, 684756, 684837, 684908, 684951, 685002, 685028, 685075, 685090, 685103, 685236, 685270, 685281, 685415, 685421, 685436, 685445, 685446, 685495, 685512, 685545, 685553, 685607, 685679, 685810, 685915, 685943, 686012, 686068, 686159, 686166, 686232, 686326, 686327, 686351, 686356, 686400, 686468, 686473, 686479, 686539, 686735, 686828, 686937, 686938, 687214, 687261, 687285, 687293, 687294, 687308, 687312, 687343, 687369, 687450, 687468, 687501, 687528, 687666, 687673, 687686, 687745, 687757, 687778, 687878, 688000, 688027, 688105, 688163, 688170, 688212, 688272, 688321, 688345, 688373, 688403, 688433, 688435, 688457, 688635, 688654, 688695, 688702, 688805, 688862, 689009, 689012, 689107, 689125, 689170, 689197, 689210, 689234, 689306, 689334, 689375, 689417, 689450, 689478, 689490, 689523, 689580, 689596, 689616, 689653, 689691, 689726, 689729, 689753, 689816, 689907, 690007, 690022, 690113, 690202, 690423, 690509, 690720, 690790, 690833, 690835, 690883, 690951, 691076, 691527, 691915, 692292, 692610, 692723, 692807, 692849, 692910, 693103, 693142, 693355, 693704, 694375, 694456, 695661, 695664, 696428, 698646, 698714, 699391, 699605, 700400, 700727, 701035, 701388, 703598, 703627, 703639, 703809, 704049, 707902, 709727, 710598, 717141, 717990, 718063, 718134, 718177, 718233, 718263, 718318, 718333, 718343, 718393, 718440, 718473, 718493, 718525, 718545, 718559, 720105, 721258, 721352, 721354, 721372, 721477, 721549, 721624, 721925, 721934, 721980, 722019, 722381, 722466, 722595, 722730, 722732, 722785, 722795, 722813, 722833, 722845, 722851, 722860, 722886, 722942, 723012, 723042, 724899, 725591, 726038, 729091, 729141, 729147, 729152, 729176, 729210, 729234, 729250, 729371, 730039, 730040, 730049, 730126, 730170, 730180, 730184, 730185, 730189, 730203, 730211, 730212, 730230, 730257, 730304, 730340, 730341, 730342, 730428, 730454, 730478, 730482, 730501, 730504, 730508, 730525, 730535, 730539, 730545, 730576, 730588, 730610, 730612, 730643, 730702, 730721, 730763, 730764, 730770, 730785, 730799, 730832, 730920, 730997, 730999, 731003, 731024, 731025, 731052, 731059, 731137, 731153, 731179, 731288, 731328, 731338, 731372, 731390, 731397, 731410, 731431, 731432, 731449, 731455, 731465, 731481, 731482, 731498, 731516, 731523, 731525, 731542, 731571, 731615, 731630, 731631, 731639, 731662, 731691, 731723, 731761, 731781, 731783, 731804, 731813, 731824, 731973, 731974, 731994, 732002, 732054, 732094, 732127, 732169, 732183, 732185, 732186, 732190, 732199, 732213, 732228, 732313, 732351, 732372, 732494, 732657, 732680, 732792, 733017, 733084, 733224, 733247, 733624, 734163, 734167, 734534, 736109, 736123, 737012, 737132, 737153, 737193, 737198, 737493, 737658, 737706, 737904, 738565, 738903, 739299, 739321, 739586,732258, 688198, 686977, 685687]
    farm_IDs = [283505,269563,269611,269852,269990,270315,270319,270582,270588,270627,270900,270939,282082,282066,282105,266896,267058,267089,267338,267492,267621,267664,267713,267889,268239,268626,269100,269458,265503,265563,265685,265704,265745,265922,266131,266237,266471,266481,266528,266576,266713,266645,266775,188536,193338,193658,193753,193913,235636,235640,235644,235765,239369,249065,261211,261289,261510,261597,261613,261740,261775,261790,262242,262385,262895,262950,263132,263275,263279,263393,263438,263485,263538,263624,263690,263764,263796,263885,263947,263972,263995,264335,264406,264649,264674,264791,264902,264910,264968,265064,265244,265377,265483,265487,188248,42634,41903,41897,41130,41843,40824,43516,40274,45053,44797,41392,166892,169128,169117,39217,169413,169439,169465,169236,169496,169503,40344,169633,169659,169742,169744,172123,181059,187745,31629,31612,31390,31365,30375,29964,29955,29830,29712,29333,19857,19723,19575,19529,19528,19337,18918,18168,29524, 29756, 29931, 31723, 31741, 31916, 32113, 32693,38858,41846, 44883, 45585, 46180, 51861, 51863, 52927,53489, 53729, 53787, 54103, 54163, 54184, 54248, 54679, 54706, 54757, 54764, 54791, 55016, 55082, 55084, 55097, 55106, 60175, 60198, 60369, 60406, 60421, 60430, 60767, 60777, 61383, 61529, 61607, 61737, 61931, 62107, 62178, 62246, 62280, 62308, 62434, 62697, 62876, 62973, 63015, 63116, 53729, 65237, 74110,75783, 76276, 76620, 77551, 77704, 77741, 79892, 80704, 80718, 81225, 84225, 84529, 84578, 84823, 85749, 86188, 86432, 86750, 87101, 87199,90465, 90961,97104,107776, 108461, 108687, 108798, 108908, 108959, 109002, 109035, 109942, 112250, 112488, 112973, 112981, 113567, 114128, 114484, 114486, 114574, 114810, 115019, 115124, 115361, 115843, 116183, 116231, 116555, 117639, 118052, 118114, 124958, 125830, 126187, 126525, 126663, 127039, 127242, 130058, 130314, 130489, 130822, 130850, 133987, 134197, 140791, 154756]

    # notfarm_IDs = [449572, 458169, 463252, 468361, 526992, 545172, 545590, 546279, 549344, 550581, 556247, 574367, 601631, 602427, 602852, 605016, 605183, 607476, 609211, 609700, 614516, 615277, 616247, 616327, 616770, 616962, 617092, 617101, 619586, 619670, 623358, 627856, 631314, 633688, 633952, 636037, 637490, 638261, 638897, 639791, 639998, 648399, 648752, 649427, 651358, 652207, 667958, 668455, 669729, 671199, 673158, 673237, 673390, 674172, 674233, 674811, 674944, 675138, 675141, 675643, 675686, 676079, 676104, 676208, 676373, 678214, 678538, 681132, 682030, 682469, 683048, 683274, 683693, 684369, 684685, 684792, 685038, 686084, 686321, 686689, 686766, 687342, 687846, 687969, 687981, 688497, 690980, 691781, 691978, 693811, 694552, 696174, 698870, 699340, 699435, 702738, 702783, 702785, 702929, 702932, 703042, 703065, 704853, 705480, 706624, 708822, 709518, 709707, 710824, 713562, 717667, 717986, 720169, 720970, 720987, 722223, 723176, 725150, 725406, 725813, 729242, 729512, 729569, 729945, 731262, 732603, 733397, 733795, 737229, 738206, 739068, 740032, 741099,693319,692188,690107,409869, 410184, 412278, 414611, 416051, 419831, 420049, 449186, 449581, 451964, 452292, 452475, 454505, 456267, 456380, 456942, 457312, 457439, 459119, 460719, 461070, 461094, 462163, 462357, 463648, 463811, 464531, 465128, 465190, 465545, 465618, 465708, 466643, 467431, 468445, 518635, 518762, 519046, 522165, 522658, 523514, 526704, 526968, 529051, 530869, 533449, 534004, 534698, 534794, 535101, 535431, 536913, 536924, 537311, 537414, 539938, 542041, 542114, 546286, 547689, 548492, 548667, 549638, 549790, 550352, 551790, 553569, 553718, 553927, 553964, 554019, 554086, 554452, 555061, 555471, 557087, 557123, 557263, 557662, 558248, 558919, 559007, 559252, 560828, 561129, 561944, 562058, 562535, 562878, 563023, 563050, 563051, 563067, 563131, 563501, 563554, 563555, 563862, 564816, 574180, 574447, 601080, 601303, 602314, 602405, 602469, 602694, 602773, 602953, 603167, 603480, 603944, 605197, 605575, 605592, 605723, 605985, 606273, 606438, 608637, 608677, 608988, 609658, 610136, 610405, 611051, 611993, 612180, 612254, 612305, 612543, 613360, 613525, 613746, 614123, 614725, 615050, 615055, 616005, 616149, 616235, 616322, 617017, 617143, 618041, 619142, 619274, 620538, 621567, 623034, 623972, 624504, 624895, 625020, 626882, 627146, 627410, 628351, 628591, 628638, 630451, 631140, 631711, 631846, 632268, 632842, 632991, 633180, 634231, 634257, 635446, 635578, 635857, 635976, 635989, 636291, 636345, 636357, 636365, 636385, 636489, 636529, 636560, 636800, 636841, 637197, 637397, 637447, 637549, 637679, 637784, 637813, 637904, 638021, 638120, 638796, 638935, 639081, 639288, 639455, 639488, 639679, 639847, 640208, 640882, 641000, 641082, 641095, 641158, 641225, 641353, 641602, 648880, 649253, 649573, 649970, 650242, 650470, 650539, 650598, 650870, 651326, 651478, 651646, 651856, 652139, 652347, 652423, 652492, 652607, 653117, 653883, 653948, 654077, 655109, 655242, 655267, 667516, 667836, 667869, 667957, 668242, 668248, 668641, 668808, 668881, 668904, 669030, 669151, 669359, 669396, 669415, 669422, 669654, 669702, 669705, 669730, 669934, 670136, 670316, 670492, 670765, 670832, 670995, 671324, 671536, 671721, 671805, 671901, 671937, 672143, 672752, 672874, 672966, 673178, 673346, 673381, 673383, 673912, 673976, 674000, 674109, 674116, 674501, 674672, 674809, 675039, 675093, 675438, 675864, 676097, 676397, 677907, 678060, 678074, 678449, 678517, 678585, 679522, 679873, 680314, 680483, 680609, 680684, 680708, 681123, 681489, 682341, 682620, 683752, 684322, 684410, 684453, 684896, 685293, 685440, 685698, 685807, 686291, 686639, 686953, 686963, 687292, 687972, 688019, 688189, 688466, 688512, 688531, 688942, 689425, 689460, 689522, 690085, 690417, 690614, 690722, 691044, 691796, 691955, 692087, 692274, 692331, 692346, 692362, 692566, 692756, 693094, 693319, 693678, 694071, 694250, 694281, 694608, 694870, 695299, 695386, 695543, 695801, 695983, 696180, 696670, 696744, 696840, 698929, 698974, 699190, 699238, 699290, 699867, 699930, 700189, 700359, 700459, 700501, 700812, 700940, 701445, 701481, 701642, 702086, 702182, 702191, 702358, 702405, 702607, 702700, 702758, 702768, 702971, 702997, 703021, 703143, 703146, 703202, 703207, 703214, 703263, 703377, 703384, 703438, 703478, 703563, 703611, 703612, 703827, 704067, 704115, 704187, 704549, 704598, 704635, 704704, 704879, 705025, 705305, 707368, 707717, 708014, 708071, 708099, 708228, 708361, 708362, 708469, 708526, 708746, 708867, 709076, 709138, 709150, 709421, 709498, 709509, 709595, 709619, 709864, 709929, 710091, 710123, 710188, 710301, 710316, 710333, 710463, 710498, 710501, 710544, 710550, 710648, 710691, 710930, 710974, 710983, 711010, 711031, 711051, 711653, 712882, 712884, 712944, 712951, 713043, 713156, 713175, 713185, 713205, 713313, 713414, 713438, 713467, 713519, 713610, 713735, 713739, 713768, 713803, 714022, 714131, 714151, 714177, 714189, 714199, 714290, 714307, 714345, 714403, 714427, 714483, 714674, 714700, 714719, 714738, 714743, 714766, 714806, 714823, 714845, 714865, 714945, 714962, 714987, 715039, 715048, 715079, 715080, 715090, 715097, 715131, 715164, 715169, 715185, 715192, 715193, 715222, 715226, 715284, 715341, 715482, 715488, 715494, 715512, 715658, 715708, 715775, 715893, 715900, 716055, 716086, 716108, 716408, 716457, 716663, 716680, 716737, 716766, 716911, 717247, 717880, 719287, 719300, 719318, 719517, 719586, 720061, 720081, 720094, 720177, 720181, 720352, 720424, 720570, 720616, 720702, 720778, 720810, 720815, 720839, 720845, 720874, 721100, 721147, 721237, 721483, 721609, 721678, 721771, 722410, 723002, 723021, 723023, 723825, 723874, 724118, 724337, 724502, 724549, 724733, 725174, 725175, 725357, 725364, 725399, 725431, 725444, 725542, 725563, 725567, 725599, 725680, 725712, 725769, 725771, 725779, 725913, 726002, 726345, 726351, 726473, 726606, 726688, 726775, 726830, 726832, 726833, 726837, 726895, 726899, 726925, 726932, 726949, 727063, 727099, 727148, 727358, 727398, 727403, 727423, 727488, 727495, 728823, 728893, 728894, 728917, 729023, 729085, 729089, 729279, 729446, 729552, 729585, 729633, 729634, 729861, 729995, 730288, 730733, 731939, 731944, 731967, 732465, 732474, 732647, 732655, 733489, 733586, 734203, 734216, 734222, 734286, 734392, 734435, 734436, 734470, 734571, 734577, 734672, 734679, 734873, 734917, 734972, 735854, 735989, 736086, 736170, 736266, 736411, 736416, 736531, 736552, 736567, 736616, 737279, 737448, 737469, 737526, 737561, 737622, 737887, 737962, 738095, 738099, 738171, 738293, 738327, 738358, 738477, 738766, 738913, 738918, 738994, 739054, 739163, 739287, 739381, 739390, 739430, 739926, 739955, 740146, 740186, 741034, 741060, 741084, 741097,620305,543433,675925,686671,622841,736637,557217,620795,719312,738058,710683,652323,722271,688785,458162,724248,650322,726463,558840,638258,736028,465121,737170,685576,709022,688169,628859,713016,620244,652304,527793,679775,640857,709954,726399,574417,723267,703426,704111,691698,682638,698943,706636,727369,573868,639020,692520,635057,725761,535060,536626,655926,703917,458503,699893,708053,724096,650724,637663,680594,720233,623917,730283,632711,461552,551030,534670,636423,702926]
    notfarm_IDs = [272342,267849,265922,261119,254430,249847,241983,204496,201623,201647,193644,193439,192500,191896,189124,188875,188720,188421,188342,188266,188142,171136,171143,171395,171587,171982,43886,172094,187117,187273,41470,187287,44845,169153,170886,169023,170395,170959,169148,171112,170406,170430,170465,170581,41376,40969,40697,27862,10714,180347,180731,181186,181209,181683,181710,181765,183305,186670,186935,187441,187467,187631,187650,187709,187772,188015,174093,174114,174126,174200 ,174218,174296,174429,174461,174519,174721,174928,175127,175434,175441,176623,177449,178662,179633,10773,172524,172530,172541,172564,172660,172750,172857,173071,173144,173181,173359,173770,173782,173885,173913,10728,1455,29374,29879,1676,29297,30478,1037,9177,2001,2078,10294,9982,2463,10354,10672,10636,349110,347555,317385,317137,314653,314135,311911,294407,287733,287442,285313,277293,277235,276938,275368,274582,274094,273694,268744,264207,263907,261179,258325,257780,257130,256830,252502,237823,235143,228331,33741,33978,149846,349110,317385,347555,92367,102569,103208,317137,53303,314653,137810,139034,314135,311911,141660,144064,294407,133626,116399,116513,127397,125214,121881,]

    total = len(level1)
    notcover = [x for x in farm_IDs if x not in level1]
    print("Didn't predict the correct labels: ",len(notcover))
    print(notcover)
    level1 = [x for x in level1 if x not in notfarm_IDs]
    print("after wrong================================")
    print(level1)
    print(len(level1))
    newprediction = [x for x in level1 if x not in farm_IDs]
    print("================================")
    print("                    newprediction: ",len(newprediction))
    print(newprediction)

    Accuracy = (len(level1)/total)
    print("Accuracy is ",Accuracy)

    
    n = n + 1







